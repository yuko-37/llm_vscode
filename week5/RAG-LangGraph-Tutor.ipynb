{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd0aad-4935-4795-93c4-0a4a5d4cbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain-text-splitters langchain-community langgraph\n",
    "!pip install -qU \"langchain[google-genai]\"\n",
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c33c81-a9c2-4533-bdc3-f83082da4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e8d58b3-e799-4e07-a255-dcac823c8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing import List, TypedDict, Literal, Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4483025-4d9f-48bc-8309-3d07f4c05b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab841ae8-64fd-4358-8e36-b4876cc1781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23cd00ae-1c6a-4def-8c7b-f6aee01d1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3370d-1b3a-4d12-bcdb-d8c25867af8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "for m in client.models.list().data:\n",
    "    if 'embedding' in m.id:\n",
    "        print(m.id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "141f9d05-9acf-443e-9a75-57d46f4fbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86a27e5a-961b-4b4a-9438-20fa02f9b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 1 docs created 63 chunks.\n",
      "{'end', 'beginning', 'middle'}\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "total_chunks = len(chunks)\n",
    "third = total_chunks // 3\n",
    "\n",
    "for i, doc in enumerate(chunks):\n",
    "    if i < third:\n",
    "        doc.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2*third:\n",
    "        doc.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        doc.metadata['section'] = \"end\"\n",
    "\n",
    "sections = set([d.metadata['section'] for d in chunks])\n",
    "\n",
    "print(f\"From {len(docs)} docs created {len(chunks)} chunks.\")\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "66b78ac2-5f91-4228-a37c-ebf4dc257d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vector store with 63 items.\n"
     ]
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore.from_documents(chunks, embeddings)\n",
    "print(f\"Created vector store with {len(vector_store.store.items())} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee90884d-2f7d-4c65-ab52-20b39cfb6b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5c4e040-2e1a-4240-a589-6e1089986c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": state[\"context\"]})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "524de519-1d85-4bfc-8e7c-e9cde33edcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer. The provided context does not contain information about RAG.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b023cd96-8e50-4583-b68d-b4d21b49d8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer based on the provided context. The retrieved documents discuss Generative Agents, ReAct, Reflexion, and AutoGPT, but do not define RAG.\n"
     ]
    }
   ],
   "source": [
    "response = await graph.ainvoke({\"question\": \"What is RAG?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0c192-fe76-4b1d-8ca0-5b23280dfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(response[\"context\"]))\n",
    "for d in response[\"context\"]:\n",
    "    print()\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3db68c6-fbaa-40b1-a217-52ac63d5b5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'section': 'beginning', 'query': 'what this article is about'}}}\n",
      "\n",
      "------------------\n",
      "\n",
      "{'retrieve': {'context': []}}\n",
      "\n",
      "------------------\n",
      "\n",
      "{'generate': {'answer': 'I cannot tell you what this article is about because no context was provided. Please provide the article or relevant text for me to summarize it.'}}\n",
      "\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What this article is about\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step}\\n\\n------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b2d922f-736e-4685-bca0-8a301c158d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['retrieve'])\n",
      "\n",
      "------------------\n",
      "\n",
      "dict_keys(['generate'])\n",
      "\n",
      "------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "async for step in graph.astream(\n",
    "       {\"question\": \"What this article is about\"}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(f\"{step.keys()}\\n\\n------------------\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15ed4522-9a7e-4cf4-9df9-ab5227c9c701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ellipsis'>\n"
     ]
    }
   ],
   "source": [
    "print(type(...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "018d814b-5043-4cc5-8e24-b89f24971366",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Search(TypedDict):\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[Literal[\"beginning\", \"middle\", \"end\"], ..., \"Section to query.\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b2be37d4-474e-4920-9b00-78ea63046234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "       filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    print(len(retrieved_docs))\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    context_docs = \"\\n\\n\".join(d.page_content for d in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": context_docs})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fb731571-46ee-41bc-996e-105546aaf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cf9ecde8-0be7-45d6-ade0-09a67bfc726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}-------------------------\n",
      "\n",
      "4\n",
      "{'retrieve': {'context': [Document(id='bded1680-1997-47b2-aeab-34affa8a2af7', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='You will get instructions for code to write.\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\nThen you will output the content of each file including ALL code.\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\nFILENAME is the lowercase file name including the file extension,\\nLANG is the markup code block language for the code’s language, and CODE is the code:\\nFILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.'), Document(id='38396b75-cf3a-44da-a924-fd5d3dd86744', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"Please now remember the steps:\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nFirst lay out the names of the core classes, functions, methods that will be necessary, As well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nPlease note that the code should be fully functional. No placeholders.\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nFollow a language and framework appropriate best practice file naming convention.\\\\nMake sure that files contain all imports, types etc. The code should be fully'), Document(id='d54ef977-8d0d-4c10-bdf2-76ea1a9d0d84', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='\"content\": \"You will get instructions for code to write.\\\\nYou will write a very long answer. Make sure that every detail of the architecture is, in the end, implemented as code.\\\\nMake sure that every detail of the architecture is, in the end, implemented as code.\\\\n\\\\nThink step by step and reason yourself to the right decisions to make sure we get it right.\\\\nYou will first lay out the names of the core classes, functions, methods that will be necessary, as well as a quick comment on their purpose.\\\\n\\\\nThen you will output the content of each file including ALL code.\\\\nEach file must strictly follow a markdown code block format, where the following tokens must be replaced such that\\\\nFILENAME is the lowercase file name including the file extension,\\\\nLANG is the markup code block language for the code\\'s language, and CODE is the code:\\\\n\\\\nFILENAME\\\\n```LANG\\\\nCODE\\\\n```\\\\n\\\\nYou will start with the \\\\\"entrypoint\\\\\" file, then go to the ones that are imported by that file, and so on.\\\\nPlease'), Document(id='b989eeb0-ae8e-41d8-963e-06cd8057f55e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='FILENAME\\nCODE\\nYou will start with the “entrypoint” file, then go to the ones that are imported by that file, and so on.\\nPlease note that the code should be fully functional. No placeholders.\\nFollow a language and framework appropriate best practice file naming convention.\\nMake sure that files contain all imports, types etc. Make sure that code in different files are compatible with each other.\\nEnsure to implement all code, if you are unsure, write a plausible implementation.\\nInclude module dependency or package manager dependency definition file.\\nBefore you finish, double check that all parts of the architecture is present in the files.\\nUseful to know:\\nYou almost always put different classes in different files.\\nFor Python, you always create an appropriate requirements.txt file.\\nFor NodeJS, you always create an appropriate package.json file.\\nYou always add a comment briefly describing the purpose of the function definition.\\nYou try to add comments explaining very complex bits of logic.')]}}-------------------------\n",
      "\n",
      "{'generate': {'answer': 'The term \"Task Decomposition\" is not explicitly mentioned in the provided text. However, the end of the post details instructions for structuring the implementation of a task, which aligns with a decomposed approach. These instructions include putting different classes in separate files, creating dependency definition files, and adding comments to describe functions and complex logic.'}}-------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}-------------------------\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68da97d-5a47-4ae4-9c89-0a3cc589c018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
